{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2244d7c",
   "metadata": {},
   "source": [
    "# ⌚️ Inferring Relationships in Wearable-Sourced Health Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3bf20",
   "metadata": {},
   "source": [
    "Ths notebook explores, models and attempts to explain latent patterns in data sourced from my Apple Watch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778a3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0558c7",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5d497",
   "metadata": {},
   "source": [
    "Define an extraction step to ingest the raw data, which we'll skil if the post-processed data is available to avoid oversharing PII."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac2ba66",
   "metadata": {},
   "source": [
    "### Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34526f2",
   "metadata": {},
   "source": [
    "Apple stores health data in a proprietary XML format which can be exported in bulk. \n",
    "\n",
    "Note: Parsing the HealthKit format below is done with the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4a77c",
   "metadata": {},
   "source": [
    "We'll limit the temporal analysis required here by summing within a calendar day. This will yield some reasonable and intuitve metrics we can try to predict off of. The consolidation of the relevant tables is surprisingly straightforward once we apply this constraint: \n",
    "\n",
    "HKCategoryTypeIdentifierSleepAnalysis\n",
    "- on end_date got range hours of sleep \n",
    "\n",
    "HKQuantityTypeIdentifierActiveEnergyBurned\n",
    "- on end_date burned value calories\n",
    "- sum value by date\n",
    "\n",
    "HKQuantityTypeIdentifierAppleExerciseTime\n",
    "- on end_date exercised for 1 min\n",
    "- sum value by end_date\n",
    "\n",
    "HKQuantityTypeIdentifierAppleStandTime\n",
    "- on end_date stood for value mins\n",
    "- sum value on end_date\n",
    "\n",
    "HKQuantityTypeIdentifierBasalEnergyBurned\n",
    "- on end_date calories burned on end_date\n",
    "- sum value on end_date\n",
    "\n",
    "HKQuantityTypeIdentifierFlightsClimbed\n",
    "- on end_date 1 flight climbed \n",
    "- sum value on end_date\n",
    "\n",
    "HKQuantityTypeIdentifierHeartRate\n",
    "- instantaneous HR measurements at end_date\n",
    "- perhaps take max value of columnd within date\n",
    "\n",
    "HKQuantityTypeIdentifierOxygenSaturation\n",
    "- instantaneous on end date\n",
    "- take max and min for the day\n",
    "\n",
    "HKQuantityTypeIdentifierPhysicalEffort\n",
    "- instanteous measure of energy on end_date\n",
    "- take max and min for the day \n",
    "\n",
    "HKQuantityTypeIdentifierRespiratoryRate\n",
    "- instantaneous \n",
    "- take average? \n",
    "\n",
    "HKQuantityTypeIdentifierRestingHeartRate\n",
    "- value has an average over a short period (length of end_date - start_date)\n",
    "- take min and max for the day \n",
    "\n",
    "HKQuantityTypeIdentifierStepCount\n",
    "- value for short window \n",
    "- sum for end_date\n",
    "\n",
    "HKQuantityTypeIdentifierTimeInDaylight\n",
    "- sum value for end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91dcc581",
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_FLAGS = (\n",
    "    \"HKCategoryTypeIdentifierSleepAnalysis\",\n",
    "    \"HKQuantityTypeIdentifierActiveEnergyBurned\",\n",
    "    \"HKQuantityTypeIdentifierAppleExerciseTime\",\n",
    "    \"HKQuantityTypeIdentifierAppleStandTime\",\n",
    "    \"HKQuantityTypeIdentifierBasalEnergyBurned\",\n",
    "    \"HKQuantityTypeIdentifierFlightsClimbed\",\n",
    "    \"HKQuantityTypeIdentifierHeartRate\",\n",
    "    \"HKQuantityTypeIdentifierOxygenSaturation\",\n",
    "    \"HKQuantityTypeIdentifierPhysicalEffort\",\n",
    "    \"HKQuantityTypeIdentifierRespiratoryRate\",\n",
    "    \"HKQuantityTypeIdentifierRestingHeartRate\",\n",
    "    \"HKQuantityTypeIdentifierStepCount\",\n",
    "    \"HKQuantityTypeIdentifierTimeInDaylight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d4076",
   "metadata": {},
   "source": [
    "❗️ The library we use for parsing HealthKit data reliese on some enum behavior that wasn't stabilized in python 3.11, however Colab is still running on 3.11 so we can't upgrade. Below GPT-5-supplied workaround to patch enum behavior is necessary if a full parsing operation is run (shoudn't be necessary if running on colab though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb22f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, enum\n",
    "\n",
    "if (3, 11) <= sys.version_info < (3, 12):\n",
    "    _old_contains = enum.EnumMeta.__contains__\n",
    "    def _patched_contains(cls, member):\n",
    "        # Try the old behavior; if 3.11 raises TypeError for str, emulate 3.12:\n",
    "        try:\n",
    "            return _old_contains(cls, member)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                cls(member)  # attempt coercion to Enum\n",
    "                return True\n",
    "            except Exception:\n",
    "                return False\n",
    "    enum.EnumMeta.__contains__ = _patched_contains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ba9d3",
   "metadata": {},
   "source": [
    "Abstract the XML-based format and emit a dict of dataframes for each of our target tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "178d28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apple_health_parser.utils.parser import Parser\n",
    "\n",
    "def parse_healthkit_data(path, output_dir=\"./data/out\", flags=INCLUDE_FLAGS): \n",
    "    \"\"\"\n",
    "    Go through the raw data with the help of the above import. \n",
    "\n",
    "    NOTE: HealthKit parsing logic courtesy of GPT-5, see https://chatgpt.com/share/68d811d3-6e00-8013-bca7-e39aa6e6f106\n",
    "    \"\"\"\n",
    "    parser = Parser(export_file=path, output_dir=output_dir, overwrite=True)\n",
    "\n",
    "    flags = [f for f in parser.flags if f.startswith(flags)]\n",
    "\n",
    "    dfs = {}      # dict[str, pd.DataFrame]  -> all parsed flags you can model from\n",
    "    errors = {}   # keep going even if a flag fails\n",
    "\n",
    "    for f in flags:\n",
    "        try:\n",
    "            parsed = parser.get_flag_records(flag=f)   # returns ParsedData\n",
    "            if not parsed.records.empty:\n",
    "                dfs[f] = parsed.records\n",
    "        except Exception as e:\n",
    "            errors[f] = repr(e)\n",
    "\n",
    "    print(len(dfs), list(dfs)[:8], len(errors))\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3bd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_date(df, date_col, val_col, op=\"sum\", name=None):\n",
    "    \"\"\"\n",
    "    Aggregate dataframe column by date, summing\n",
    "\n",
    "    NOTE:  Date aggregation syntax courtesy of gpt-5, see https://chatgpt.com/share/68d82165-543c-8013-a1c4-4d5ac9d68410\n",
    "    \"\"\"\n",
    "    groups = df.groupby(df[date_col].dt.date)\n",
    "        \n",
    "    agg_df = None\n",
    "    match(op):         \n",
    "        case \"min\" : \n",
    "            agg_df = groups[val_col].min().reset_index()\n",
    "        case \"max\" : \n",
    "            agg_df = groups[val_col].max().reset_index()\n",
    "        case \"mean\" : \n",
    "            agg_df = groups[val_col].mean().reset_index()\n",
    "        case \"sum\" | None | _ : \n",
    "            agg_df = groups[val_col].sum().reset_index()\n",
    "\n",
    "    kwargs = { 'columns': [name] } if name else {}\n",
    "    return pd.DataFrame(agg_df[val_col].values, index=agg_df[date_col].values, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592f5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_healthkit_data(table_dfs):\n",
    "    \"\"\"\n",
    "    Given a dict of dataframes with our target data container, filter and construct a unifed dataframe \n",
    "    that aggregates data by date. Note the tight coupling with parse flags above. \n",
    "    \"\"\"\n",
    "\n",
    "    agg_dfs = {}\n",
    "    agg_dfs['sleep'] = aggregate_by_date(table_dfs['HKCategoryTypeIdentifierSleepAnalysis'], date_col=\"end_date\", val_col=\"range\", op=\"sum\", name=\"sleep_h\") \n",
    "    agg_dfs['energy'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierActiveEnergyBurned'], date_col=\"end_date\", val_col=\"value\", op=\"sum\", name=\"active_cal\") \n",
    "    agg_dfs['exercise'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierAppleExerciseTime'], date_col=\"end_date\", val_col=\"value\", op=\"sum\", name=\"exercise_h\") \n",
    "    agg_dfs['stand'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierAppleStandTime'], date_col=\"end_date\", val_col=\"value\", op=\"sum\", name=\"stand_h\") \n",
    "    agg_dfs['basal'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierBasalEnergyBurned'], date_col=\"end_date\", val_col=\"value\", op=\"sum\", name=\"basal_cal\") \n",
    "    agg_dfs['flights'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierFlightsClimbed'], date_col=\"end_date\", val_col=\"value\", op=\"sum\", name=\"flights\") \n",
    "    agg_dfs['hr'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierHeartRate'], date_col=\"end_date\", val_col=\"value\", op=\"max\", name=\"max_hr\") \n",
    "    agg_dfs['o2'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierOxygenSaturation'], date_col=\"end_date\", val_col=\"value\", op=\"min\", name=\"min_o2\") \n",
    "    agg_dfs['effort'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierPhysicalEffort'], date_col=\"end_date\", val_col=\"value\", op=\"sum\", name=\"activity_mets\") \n",
    "    agg_dfs['rr'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierRespiratoryRate'], date_col=\"end_date\", val_col=\"value\", op=\"mean\", name=\"avg_rr\") \n",
    "    agg_dfs['resting'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierRestingHeartRate'], date_col=\"end_date\", val_col=\"value\", op=\"mean\", name=\"avg_resting_hr\") \n",
    "    agg_dfs['step'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierStepCount'], date_col=\"end_date\", val_col=\"value\", op=\"sum\", name=\"steps\") \n",
    "    agg_dfs['daylight'] = aggregate_by_date(table_dfs['HKQuantityTypeIdentifierTimeInDaylight'], date_col=\"end_date\", val_col=\"value\", op=\"sum\", name=\"daylight_h\") \n",
    "\n",
    "    # Migrat to float value for sleep duration from timedelta.  NOTE: GPT-5-supplied conversion syntax\n",
    "    agg_dfs['sleep'].sleep_h = agg_dfs['sleep'].sleep_h / np.timedelta64(1, \"h\")\n",
    "\n",
    "    # Join our table DFs\n",
    "    df = pd.DataFrame()\n",
    "    df = df.join(list(agg_dfs.values()), how=\"outer\")\n",
    "    #macro_df.reset_index(names='date', inplace=True)\n",
    "\n",
    "    # Coax the string value for the date back to a numeric and sort. NOTE: type conversion courtesy of GPT-5\n",
    "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69d667",
   "metadata": {},
   "source": [
    "If we can restore the processed data, do so, otherwise mine our data out of the full HealthKit export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d4b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = \"./data/export.zip\"\n",
    "processed_dataset = \"./data/health.csv\"\n",
    "\n",
    "df = pd.read_csv(processed_dataset)\n",
    "if df is None: \n",
    "    \n",
    "    # Mine the raw export \n",
    "    table_dfs = parse_healthkit_data(raw_dataset)\n",
    "    df = process_healthkit_data(table_dfs)\n",
    "\n",
    "    # Inspect for spsarsity\n",
    "    sns.heatmap(df.isna())\n",
    "    \n",
    "    # Avoid doing this again on subsequent runs!\n",
    "    df.to_csv(\"dataset_path\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb4642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
